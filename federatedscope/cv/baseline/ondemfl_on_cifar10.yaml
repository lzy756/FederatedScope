use_gpu: True
device: 0
seed: 12345
early_stop:
  patience: 10

federate:
  mode: standalone
  method: ondemfl
  client_num: 100
  sample_client_num: 10
  total_round_num: 300  # pretrain_rounds + ondemand_rounds
  make_global_eval: True
  merge_test_data: True

data:
  root: data/
  type: CIFAR10@torchvision
  splits: [1.0, 0.0, 0.0]
  num_workers: 0
  transform:
    - [ToTensor]
    - [Normalize, {'mean': [0.4914, 0.4822, 0.4465], 'std': [0.2470, 0.2435, 0.2616]}]
  test_transform:
    - [ToTensor]
    - [Normalize, {'mean': [0.4914, 0.4822, 0.4465], 'std': [0.2470, 0.2435, 0.2616]}]
  args:
    - {'download': True}
  splitter: lda
  splitter_args:
    - {'alpha': 0.5}

dataloader:
  batch_size: 64

model:
  type: ondemfl
  out_channels: 10
  backbone: convnet2
  backbone_config:
    hidden: 2048
    dropout: 0.0
  distribution_hidden: [1024, 256]
  distribution_temperature: 1.0

train:
  local_update_steps: 1
  batch_or_epoch: epoch
  optimizer:
    type: SGD
    lr: 0.01
    momentum: 0.9
    weight_decay: 0.0

grad:
  grad_clip: 5.0

criterion:
  type: CrossEntropyLoss

trainer:
  type: ondemfl

ondemfl:
  enable: True
  pretrain_rounds: 150
  ondemand_rounds: 150
  target_distribution: [0.2, 0.12, 0.1, 0.1, 0.1, 0.08, 0.08, 0.07, 0.07, 0.08]
  subset_size: 12
  weight_scheme: ratio_times_size
  dp_loss: mse
  dp_optimizer:
    type: SGD
    lr: 0.005
    momentum: 0.9
    weight_decay: 0.0
  nnls_max_iter: 500
  nnls_tol: 1e-9

eval:
  freq: 1
  metrics: [acc, correct]
  best_res_update_round_wise_key: test_loss
