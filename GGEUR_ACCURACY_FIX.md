# GGEUR å‡†ç¡®ç‡é—®é¢˜è¯Šæ–­å’Œä¿®å¤

## å‘ç°çš„å…³é”®é—®é¢˜

é€šè¿‡å¯¹æ¯”æºç  `D:\Projects\2025CVPR_GGEUR\` å‘ç°äº†å¯¼è‡´å‡†ç¡®ç‡ä½çš„**3ä¸ªå…³é”®å·®å¼‚**ï¼š

### é—®é¢˜ 1ï¼šç‰¹å¾å€¼ç¼©æ”¾é¡ºåºé”™è¯¯ âŒ **ä¸¥é‡**

**æºç å®ç°**ï¼ˆæ­£ç¡®çš„ï¼Œè™½ç„¶çœ‹èµ·æ¥å¥‡æ€ªï¼‰ï¼š
```python
# D:\Projects\2025CVPR_GGEUR\Multi Domain\Office-Home-LDS\prototype_cov_matrix_generate_features.py
def nearest_pos_def(cov_matrix):
    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)  # è¿”å›å‡åºï¼š[å° -> å¤§]
    scale_factors = np.ones_like(eigenvalues)
    scale_factors[:10] = np.linspace(5, 1, 10)  # ç¼©æ”¾æœ€å°çš„10ä¸ªï¼
    eigenvalues = eigenvalues * scale_factors
    eigenvalues[eigenvalues < 0] = 0
    return eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T
```

**æˆ‘ä»¬çš„å®ç°**ï¼ˆé”™è¯¯ï¼‰ï¼š
```python
# federatedscope/contrib/utils/geometry_utils.py
def eigendecompose(...):
    eigenvalues, eigenvectors = torch.linalg.eigh(covariance_matrix)  # å‡åº
    # ç„¶åæ’åºæˆé™åºï¼
    sorted_indices = torch.argsort(eigenvalues, descending=True)
    eigenvalues = eigenvalues[sorted_indices]  # [å¤§ -> å°]
    eigenvectors = eigenvectors[:, sorted_indices]
    # ...

def _apply_eigenvalue_scaling(eigenvalues):
    scale_factors[:n_scale] = torch.linspace(5.0, 1.0, n_scale)
    # å®é™…ç¼©æ”¾çš„æ˜¯æœ€å¤§çš„10ä¸ªï¼âŒ
```

**é—®é¢˜è§£é‡Š**ï¼š
- æºç ï¼š`np.linalg.eigh` è¿”å›å‡åºç‰¹å¾å€¼ï¼Œç¼©æ”¾ `[:10]` = ç¼©æ”¾æœ€å°çš„10ä¸ª
- æˆ‘ä»¬ï¼šå…ˆé™åºæ’åºï¼Œç„¶åç¼©æ”¾ `[:10]` = ç¼©æ”¾æœ€å¤§çš„10ä¸ª
- **è¿™å®Œå…¨æ”¹å˜äº†ç®—æ³•çš„è¡Œä¸ºï¼**

### é—®é¢˜ 2ï¼šæ¨¡å‹è¾“å‡ºå±‚å®šä¹‰ä¸åŒ âŒ **é‡è¦**

**æºç å®ç°**ï¼š
```python
# D:\Projects\2025CVPR_GGEUR\Multi Domain\Office-Home-LDS\FedAvg_GGEUR.py
class MyNet(nn.Module):
    def __init__(self, num_classes=65):
        super(MyNet, self).__init__()
        self.fc3 = nn.Linear(512, num_classes)

    def forward(self, x):
        return F.softmax(self.fc3(x), dim=1)  # è¿”å›æ¦‚ç‡ï¼

# è®­ç»ƒæ—¶
criterion = nn.CrossEntropyLoss()  # å†…éƒ¨ä¼šå†åšä¸€æ¬¡ log_softmax
```

**æˆ‘ä»¬çš„å®ç°**ï¼š
```python
# federatedscope/contrib/trainer/ggeur_trainer.py
class MLP:
    def forward(self, x):
        return self.fc(x)  # è¿”å› logits

# è®­ç»ƒæ—¶
criterion = nn.CrossEntropyLoss()  # è¿™æ˜¯æ­£ç¡®çš„ç”¨æ³•
```

**é—®é¢˜è§£é‡Š**ï¼š
- æºç ï¼šforward è¿”å› softmax ç»“æœï¼Œç„¶å CrossEntropyLoss å†…éƒ¨å†åš log_softmaxï¼ˆè™½ç„¶è¿™åœ¨æŠ€æœ¯ä¸Šæ˜¯é”™è¯¯çš„ï¼‰
- æˆ‘ä»¬ï¼šforward è¿”å› logitsï¼ŒCrossEntropyLoss åš log_softmaxï¼ˆè¿™æ˜¯æ ‡å‡†ç”¨æ³•ï¼‰
- **è™½ç„¶æºç çš„ç”¨æ³•ä¸æ ‡å‡†ï¼Œä½†æˆ‘ä»¬å¿…é¡»å¤ç°å®ƒæ‰èƒ½å¾—åˆ°ç›¸åŒç»“æœ**

### é—®é¢˜ 3ï¼šåæ–¹å·®çŸ©é˜µå½’ä¸€åŒ– âš ï¸ **å¯èƒ½**

**æºç **ï¼ˆç¬¬60è¡Œï¼‰ï¼š
```python
# é™¤ä»¥ n_samplesï¼ˆæœ‰åä¼°è®¡ï¼‰
covariance = torch.mm(centered.t(), centered) / n_samples
```

**æˆ‘ä»¬**ï¼ˆgeometry_utils.py:60ï¼‰ï¼š
```python
# é™¤ä»¥ n_samplesï¼ˆåŒæ ·æ˜¯æœ‰åä¼°è®¡ï¼‰
covariance = torch.mm(centered.t(), centered) / n_samples
```

è¿™ä¸ªæ˜¯ä¸€è‡´çš„ï¼Œä¸æ˜¯é—®é¢˜ã€‚

---

## ä¿®å¤æ–¹æ¡ˆ

### ä¿®å¤ 1ï¼šç‰¹å¾å€¼ç¼©æ”¾é¡ºåº

éœ€è¦ä¿®æ”¹ `federatedscope/contrib/utils/geometry_utils.py`ï¼š

#### æ–¹æ¡ˆAï¼šä¸æ’åºï¼Œç›´æ¥ç”¨å‡åºï¼ˆä¸æºç å®Œå…¨ä¸€è‡´ï¼‰

```python
def eigendecompose(covariance_matrix: torch.Tensor,
                  top_k: Optional[int] = None,
                  min_eigenvalue: float = 1e-8) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Perform eigenvalue decomposition on covariance matrix.

    NOTE: For GGEUR_Clip, we keep eigenvalues in ASCENDING order (as returned by eigh)
    to match the paper's implementation which scales the SMALLEST 10 eigenvalues.
    """
    # Eigenvalue decomposition (returns ASCENDING order)
    eigenvalues, eigenvectors = torch.linalg.eigh(covariance_matrix)

    # DO NOT SORT! Keep ascending order to match paper's nearest_pos_def
    # scale_factors[:10] will scale the SMALLEST 10 eigenvalues

    # Filter out very small eigenvalues
    valid_mask = eigenvalues > min_eigenvalue
    eigenvalues = eigenvalues[valid_mask]
    eigenvectors = eigenvectors[:, valid_mask]

    if top_k is not None and top_k < len(eigenvalues):
        # Take LAST k (largest) for top_k
        eigenvalues = eigenvalues[-top_k:]
        eigenvectors = eigenvectors[:, -top_k:]

    return eigenvalues, eigenvectors
```

#### æ–¹æ¡ˆBï¼šç»§ç»­é™åºï¼Œä½†ä¿®æ”¹ç¼©æ”¾é€»è¾‘

```python
def _apply_eigenvalue_scaling(eigenvalues: torch.Tensor) -> torch.Tensor:
    """
    Apply eigenvalue scaling as in paper's nearest_pos_def function.

    NOTE: Paper scales the SMALLEST 10 eigenvalues (because eigh returns ascending).
    Since we sort to descending, we need to scale the LAST 10.
    """
    eigenvalues = eigenvalues.clone()
    scale_factors = torch.ones_like(eigenvalues)

    # Scale SMALLEST 10 eigenvalues (at the end after descending sort)
    n_scale = min(10, len(eigenvalues))
    scale_factors[-n_scale:] = torch.linspace(5.0, 1.0, n_scale,
                                              dtype=eigenvalues.dtype,
                                              device=eigenvalues.device)

    scaled_eigenvalues = eigenvalues * scale_factors
    return torch.clamp(scaled_eigenvalues, min=0)
```

**æ¨èæ–¹æ¡ˆA**ï¼Œå› ä¸ºæ›´ç®€å•ä¸”ä¸æºç å®Œå…¨ä¸€è‡´ã€‚

### ä¿®å¤ 2ï¼šæ¨¡å‹è¾“å‡ºå±‚

éœ€è¦ä¿®æ”¹ `federatedscope/contrib/trainer/ggeur_trainer.py`ï¼š

```python
class GGEURTrainer(GeneralTorchTrainer):

    def _build_mlp_classifier(self):
        """Build MLP classifier for embedding space."""
        cfg = self._cfg.ggeur

        input_dim = cfg.embedding_dim
        num_classes = self._cfg.model.num_classes
        num_layers = cfg.get('mlp_layers', 1)

        if num_layers == 1:
            # Single layer MLP (linear classifier)
            # Wrap with softmax to match paper's implementation
            self.mlp_classifier = nn.Sequential(
                nn.Linear(input_dim, num_classes),
                nn.Softmax(dim=1)  # æ·»åŠ  softmaxï¼
            ).to(self.ctx.device)
        else:
            # Multi-layer MLP
            layers = []
            # ... (hidden layers)
            layers.append(nn.Linear(hidden_dim, num_classes))
            layers.append(nn.Softmax(dim=1))  # æ·»åŠ  softmaxï¼
            self.mlp_classifier = nn.Sequential(*layers).to(self.ctx.device)
```

### ä¿®å¤ 3ï¼šè®­ç»ƒå‚æ•°æ£€æŸ¥

ç¡®ä¿é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°ä¸æºç ä¸€è‡´ï¼š

**Office-Home**ï¼ˆæºç ç¬¬223è¡Œï¼‰ï¼š
```yaml
train:
  local_update_steps: 1  # åªè®­ç»ƒ1ä¸ªepochï¼
  optimizer:
    type: 'Adam'
    lr: 0.001
dataloader:
  batch_size: 16
federate:
  total_round_num: 50
```

**Digits**ï¼ˆæºç è®­ç»ƒå¾ªç¯ï¼‰ï¼š
```yaml
train:
  local_update_steps: 10  # 10ä¸ªepoch
  optimizer:
    type: 'Adam'
    lr: 0.01
dataloader:
  batch_size: 16
federate:
  total_round_num: 50
```

---

## ä¿®å¤ä¼˜å…ˆçº§

### ğŸ”´ ç«‹å³ä¿®å¤ï¼ˆå½±å“å·¨å¤§ï¼‰ï¼š
1. **ç‰¹å¾å€¼ç¼©æ”¾é¡ºåº**ï¼ˆé—®é¢˜1ï¼‰ - è¿™ä¸ªé”™è¯¯ä¼šå®Œå…¨æ”¹å˜æ•°æ®å¢å¼ºçš„æ•ˆæœ
2. **æ¨¡å‹è¾“å‡ºå±‚**ï¼ˆé—®é¢˜2ï¼‰ - å½±å“è®­ç»ƒåŠ¨æ€å’Œæ”¶æ•›

### ğŸŸ¡ æ£€æŸ¥ç¡®è®¤ï¼š
3. è®­ç»ƒå‚æ•°ï¼ˆlocal_epochs, optimizer, lr, batch_sizeï¼‰
4. æ•°æ®å¢å¼ºå‚æ•°ï¼ˆnum_per_sample, target_sizeï¼‰

---

## é¢„æœŸå½±å“

ä¿®å¤è¿™äº›é—®é¢˜åï¼Œå‡†ç¡®ç‡åº”è¯¥èƒ½è¾¾åˆ°è®ºæ–‡æ°´å¹³ï¼š

**Office-Home**ï¼ˆè®ºæ–‡ Table 4ï¼‰ï¼š
- Art: ~65%
- Clipart: ~52%
- Product: ~77%
- Real_World: ~79%
- Average: ~68%

**Digits**ï¼ˆè®ºæ–‡ Table 4ï¼‰ï¼š
- MNIST: ~97%
- USPS: ~94%
- SVHN: ~63%
- SYN: ~75%
- Average: ~82%

**PACS**ï¼ˆè®ºæ–‡ Table 3ï¼‰ï¼š
- Photo: ~95%
- Art_Painting: ~84%
- Cartoon: ~83%
- Sketch: ~82%
- Average: ~86%
