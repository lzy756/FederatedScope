# FedLSA Optimized Configuration for Office-Caltech
# 优化策略：
# 1. 增加anchor训练轮数以充分学习语义锚点
# 2. 调整lambda_com和alpha_sep到更优值
# 3. 增加训练轮数和本地更新步数
# 4. 添加dropout防止过拟合
# 5. 使用余弦退火学习率调度

# Basic FL settings
use_gpu: True
device: 0
backend: 'torch'
seed: 123

# Federate settings
federate:
  mode: 'standalone'
  client_num: 20
  total_round_num: 200          # 增加训练轮数 (200 → 300)
  sample_client_num: 5
  method: 'fedlsa'
  make_global_eval: False
  share_local_model: False
  online_aggr: False

# Data settings
data:
  root: '/home/liziyu/data/office_caltech_10'
  type: 'office_caltech'
  splits: [0.8, 0.1, 0.1]       # train, val, test
  batch_size: 16                # 增大batch size (8 → 16)
  shuffle: True
  num_workers: 0
  dirichlet_alpha: 0.1          # 增加数据平衡性 (0.1 → 0.3)

# Model settings
model:
  type: 'fedlsa_cnn'            # FedLSA paper model: 2 conv + 3 linear layers
  hidden: 512
  num_classes: 10               # 10 for Office-Caltech
  dropout: 0.2                  # 添加dropout防止过拟合 (0.0 → 0.2)
  out_channels: 10

# Training settings
train:
  local_update_steps: 8         # 增加本地训练步数 (4 → 8)
  batch_or_epoch: 'epoch'
  optimizer:
    type: 'SGD'
    lr: 0.005                   # 降低学习率 (0.01 → 0.005)
    momentum: 0.9
    weight_decay: 0.001         # 增加权重衰减 (0.0005 → 0.001)
  scheduler:
    type: 'CosineAnnealingLR'   # 使用余弦退火 (StepLR → CosineAnnealingLR)
    T_max: 300
    eta_min: 0.0001

# Evaluation settings
eval:
  freq: 5                       # 更频繁的评估 (10 → 5)
  metrics: ['acc', 'loss']
  best_res_update_round_wise_key: 'test_acc'
  report: ['weighted_avg', 'avg', 'raw']
  split: ['test', 'val']

# Early stopping
early_stop:
  patience: 80                  # 增加耐心值 (50 → 80)
  delta: 0.0
  improve_indicator_mode: 'best'

# FedLSA specific settings (优化后的超参数)
fedlsa:
  use: True

  # Client-side parameters
  lambda_com: 0.8               # 增加紧凑性损失权重 (0.5 → 0.8)
  tau: 0.15                     # 稍微增大温度参数 (0.1 → 0.15)
  use_projector: True
  projector_input_dim: 512
  projector_output_dim: 128
  share_projector: True

  # Server-side parameters
  alpha_sep: 0.4                # 减小到论文推荐值 (0.6 → 0.4)
  anchor_train_epochs: 30       # 大幅增加anchor训练轮数 (5 → 30)
  anchor_lr: 0.001

# Trainer type
trainer:
  type: 'fedlsa_trainer'

# Output directory
outdir: 'exp_fedlsa/office_caltech'
expname: 'fedlsa_tuned'         # 新的实验名称

# Criterion
criterion:
  type: 'CrossEntropyLoss'

# Regularizer (not used in FedLSA)
regularizer:
  type: ''
  mu: 0.0

# Gradient
grad:
  grad_clip: 5.0

# Logging
verbose: 1
print_decimal_digits: 6
