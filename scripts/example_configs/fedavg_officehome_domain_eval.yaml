# FedAvg Configuration for Office-Home Dataset
# This uses standard FedAvg training (no GGEUR_Clip augmentation)
# Matches ggeur_officehome_lds.yaml in terms of dataset and domain distribution

# Basic settings
use_gpu: True
device: 1
backend: 'torch'
seed: 12345

# Federate settings
federate:
  mode: 'standalone'
  client_num: 4  # Same as GGEUR_Clip config (4 domains)
  total_round_num: 50  # Same as GGEUR_Clip config
  method: 'fedavg_domain_eval'  # Use FedAvg with domain evaluation
  make_global_eval: True  # Enable server-side evaluation
  share_local_model: False
  online_aggr: False
  sample_client_num: 4  # Sample all clients each round

# Data settings - Same as ggeur_officehome_lds.yaml
data:
  root: 'OfficeHomeDataset_10072016/'
  type: 'OfficeHome'
  splits: [0.8, 0.1, 0.1]  # train, val, test
  splitter: 'lda_domain'  # Same as GGEUR_Clip config
  splitter_args:
    - alpha: 0.1  # Same as GGEUR_Clip config

  # Server test set configuration
  server_test_ratio: 0.2  # Use 20% of each domain for server test set

# DataLoader settings
dataloader:
  batch_size: 16  # Same as GGEUR_Clip config
  shuffle: True
  num_workers: 0

# Model settings - Use CNN instead of MLP (standard FedAvg approach)
model:
  type: 'fedlsa_cnn'
  hidden: 512
  num_classes: 65  # Office-Home has 65 classes
  dropout: 0.0
  out_channels: 65

# Training settings
train:
  local_update_steps: 1  # Same as GGEUR_Clip config
  batch_or_epoch: 'epoch'
  optimizer:
    type: 'Adam'  # Same as GGEUR_Clip config
    lr: 0.001  # Same as GGEUR_Clip config
    weight_decay: 0.0  # Same as GGEUR_Clip config

# Evaluation settings
eval:
  freq: 1
  metrics: ['acc']
  best_res_update_round_wise_key: 'test_acc'
  split: ['test']

# Early stopping
early_stop:
  patience: 50
  delta: 0.0
  improve_indicator_mode: 'best'

# Trainer type - Use general trainer for FedAvg
trainer:
  type: 'general'

# Criterion
criterion:
  type: 'CrossEntropyLoss'

# Output directory
outdir: 'exp/fedavg_officehome_domain_eval'
expname: 'fedavg_officehome_beta0.1'

# Gradient clipping
grad:
  grad_clip: 5.0
