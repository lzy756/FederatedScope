# FedAvg baseline for Office-Caltech (same data/hyper-parameters as CDA setup)
use_gpu: True
device: 0
backend: 'torch'
seed: 123

federate:
  mode: 'standalone'
  client_num: 20
  total_round_num: 300
  sample_client_num: 5
  method: 'fedavg'
  make_global_eval: False
  share_local_model: False
  online_aggr: False

data:
  root: 'data/office_caltech_10'
  type: 'office_caltech'
  splits: [0.8, 0.1, 0.1]
  batch_size: 4
  shuffle: True
  num_workers: 0
  dirichlet_alpha: 0.1

model:
  type: 'fedlsa_cnn'
  hidden: 512
  num_classes: 10
  dropout: 0.0
  out_channels: 10

train:
  local_update_steps: 4
  batch_or_epoch: 'epoch'
  optimizer:
    type: 'SGD'
    lr: 0.01
    momentum: 0.9
    weight_decay: 0.0005
  scheduler:
    type: 'StepLR'
    step_size: 30
    gamma: 0.1

eval:
  freq: 1
  metrics: ['acc', 'loss']
  best_res_update_round_wise_key: 'test_acc'
  report: ['weighted_avg', 'avg', 'raw']
  split: ['test', 'val']

early_stop:
  patience: 0
  delta: 0.0
  improve_indicator_mode: 'best'

trainer:
  type: 'general'

criterion:
  type: 'CrossEntropyLoss'

grad:
  grad_clip: 5.0


# verbose: 1
# print_decimal_digits: 6
