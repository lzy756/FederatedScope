# GGEUR_Clip with CNN Knowledge Distillation - Office-Home LDS
# This configuration enables CNN training with knowledge distillation from MLP teacher
#
# Training flow:
# 1. Extract CLIP features and train MLP (standard GGEUR_Clip)
# 2. Use MLP as teacher to train CNN with knowledge distillation
# 3. CNN is trained on original images with soft labels from MLP
#
# Loss function for CNN:
# L = alpha * CE(CNN(x), y) + (1-alpha) * T^2 * KL(softmax(CNN(x)/T), softmax(MLP(CLIP(x))/T))

# ========== Basic Settings ==========
use_gpu: True
device: 1
seed: 42
verbose: 1

# ========== Federated Learning Settings ==========
federate:
  method: 'ggeur'
  mode: 'standalone'
  client_num: 4  # One client per domain
  total_round_num: 50
  sample_client_num: 4

# ========== Data Settings ==========
data:
  type: 'office-home'
  root: 'OfficeHomeDataset_10072016'
  splits: [0.7, 0.0, 0.3]

dataloader:
  batch_size: 16
  num_workers: 0

# ========== Model Settings ==========
model:
  type: 'ggeur_mlp'
  num_classes: 65

# ========== Training Settings ==========
train:
  local_update_steps: 10
  optimizer:
    type: 'Adam'
    lr: 0.001

# ========== GGEUR_Clip Specific Settings ==========
ggeur:
  use: True

  # CLIP Feature Extraction
  clip_model: 'ViT-B-16'
  clip_pretrained: 'openai'
  clip_model_path: '/root/model/open_clip_vitb16.bin'
  embedding_dim: 512

  # Feature Caching
  use_feature_cache: True
  feature_cache_dir: ''

  # Feature Augmentation
  num_generated_per_sample: 50
  num_generated_per_prototype: 50
  target_size_per_class: 50

  # MLP Classifier (teacher model)
  mlp_hidden_dim: 0
  mlp_dropout: 0.0

  # Multi-domain Settings
  use_cross_client_prototypes: True

  # Training Settings
  statistics_round: 0

  # LDS Settings
  use_lds: True
  lds_alpha: 0.1
  lds_seed: 42

  # ========== CNN Knowledge Distillation Settings ==========
  use_cnn_distillation: True  # Enable CNN training

  # CNN model architecture
  cnn_model: 'resnet18'  # Options: resnet18, resnet34, resnet50, mobilenet_v2
  cnn_pretrained: True   # Use ImageNet pretrained weights

  # Knowledge distillation parameters
  distill_temperature: 4.0  # Temperature for soft labels (higher = softer)
  distill_alpha: 0.5        # Weight: 0.5 means equal CE and KL loss

  # CNN training settings
  cnn_lr: 0.01              # Learning rate for CNN (higher for fine-tuning)
  cnn_local_epochs: 10      # Local epochs for CNN training per round
  cnn_warmup_rounds: 2      # Skip CNN distillation in first N rounds (wait for MLP to learn)

# ========== Output Settings ==========
outdir: 'exp/ggeur_cnn_distillation'
expname: 'ggeur_cnn_resnet18'

