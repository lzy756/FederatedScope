# FedAvg Configuration for Domain-Skewed Federated Learning
# Dataset: Office-Caltech (baseline comparison with FedLSA)
# Pure FedAvg without any FedLSA-specific components

# Basic FL settings
use_gpu: True
device: 0
backend: 'torch'
seed: 123

# Federate settings
federate:
  mode: 'standalone'
  client_num: 20
  total_round_num: 100
  sample_client_num: 5
  method: 'fedavg'
  make_global_eval: False
  share_local_model: False
  online_aggr: False

# Data settings
data:
  root: '/home/liziyu/data/office_caltech_10'
  type: 'office_caltech'
  splits: [0.8, 0.1, 0.1]  # train, val, test
  batch_size: 4
  shuffle: True
  num_workers: 0
  dirichlet_alpha: 0.1  # Dirichlet concentration parameter for class heterogeneity
                        # 0.0 = uniform split (no heterogeneity)
                        # 0.1 = highly heterogeneous (each client has few classes)
                        # 0.5 = moderate heterogeneity
                        # 1.0 = more balanced
                        # 10.0 = almost uniform distribution

# Model settings - using same CNN architecture for fair comparison
model:
  type: 'fedlsa_cnn'
  hidden: 512
  num_classes: 10
  dropout: 0.3
  out_channels: 10

# Training settings
train:
  local_update_steps: 1
  batch_or_epoch: 'epoch'
  optimizer:
    type: 'SGD'
    lr: 0.0001
    momentum: 0.9
    weight_decay: 0.0005
  scheduler:
    type: 'StepLR'
    step_size: 10
    gamma: 0.1

# Evaluation settings
eval:
  freq: 1
  metrics: ['acc', 'loss']
  best_res_update_round_wise_key: 'test_acc'
  report: ['weighted_avg', 'avg', 'raw']
  split: ['test', 'val']

# Early stopping
early_stop:
  patience: 50
  delta: 0.0
  improve_indicator_mode: 'best'

# Trainer type
trainer:
  type: 'general'

# Output directory
outdir: 'exp_fedavg/office_caltech'
expname: 'fedavg_baseline'

# Criterion
criterion:
  type: 'CrossEntropyLoss'

# Regularizer
regularizer:
  type: ''
  mu: 0.0

# Gradient
grad:
  grad_clip: 5.0

# Logging
verbose: 1
print_decimal_digits: 6




