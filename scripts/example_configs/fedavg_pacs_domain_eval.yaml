# FedAvg Configuration for Office-Caltech-10 with Domain-Specific Evaluation
# This configuration uses FedAvg training method but includes domain-specific
# evaluation logic similar to cross_domain_adaptive for fair comparison

# Basic settings
use_gpu: True
device: 0
backend: 'torch'
seed: 123

# Federate settings
federate:
  mode: 'standalone'
  client_num: 50
  total_round_num: 200
  method: 'fedavg_domain_eval'  # Use our custom method with domain evaluation
  make_global_eval: True  # Enable server-side evaluation
  share_local_model: False
  online_aggr: False

# Data settings - Same as cross_domain_adaptive
data:
  root: '/root/data/PACS'
  type: 'pacs'
  splits: [0.8, 0.1, 0.1]  # train, val, test
  dirichlet_alpha: 0.1  # Same heterogeneity as cross_domain_adaptive
  server_test_samples_per_class: 40  # 20 samples/class * 10 classes = 200 samples/domain

# DataLoader settings
dataloader:
  batch_size: 8
  shuffle: True
  num_workers: 0

# Model settings - Same as cross_domain_adaptive for fair comparison
model:
  type: 'fedlsa_cnn'
  hidden: 512
  num_classes: 7
  dropout: 0.0
  out_channels: 7

# Training settings - Same as cross_domain_adaptive
train:
  local_update_steps: 4
  batch_or_epoch: 'epoch'
  optimizer:
    type: 'SGD'
    lr: 0.01
    momentum: 0.9
    weight_decay: 0.0005
  scheduler:
    type: 'StepLR'
    step_size: 100
    gamma: 0.1

# Evaluation settings
eval:
  freq: 1
  metrics: ['acc', 'loss']
  best_res_update_round_wise_key: 'test_acc'
  report: ['weighted_avg', 'avg', 'raw']
  split: ['test', 'val']

# Early stopping
early_stop:
  patience: 0
  delta: 0.0
  improve_indicator_mode: 'best'

# Trainer type - Use general trainer for FedAvg
trainer:
  type: 'general'

# Criterion
criterion:
  type: 'CrossEntropyLoss'

# Gradient clipping
grad:
  grad_clip: 5.0


